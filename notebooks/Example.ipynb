{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on using the Beyond Quantum Noise Spectroscopy Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preample\n",
    "from ipywidgets import widgets\n",
    "from itertools import product\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./../src/')\n",
    "from qubitmlmodel import  qubitMLmodel\n",
    "from datasets_cat1 import sim_CPMG_G_SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a dataset (skip if the dataset is already generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the simulation parameters\n",
    "T      = 1         # total time        \n",
    "M      = 4096      # number of discrete time steps\n",
    "Omega  = 10        # energy gap of the qubit\n",
    "\n",
    "###########################################################################   \n",
    "# generate single-axis dataset of Gaussian pulses \n",
    "print(\"Generating single-axis dataset of Gaussian Pulses\")\n",
    "\n",
    "# initialize the random number generator at some known point for reproducability\n",
    "np.random.seed(seed = 40)\n",
    "\n",
    "n_max       = 1  # maximum number of pulses \n",
    "parameters  = [(T, M, Omega, 1, 0,0, n_max)] + [(T, M, Omega, n_x, np.random.rand()*2, (2*np.random.rand(n_x)-1), n_max) for n_x,_ in product(range(1,n_max+1),range(75))]\n",
    "\n",
    "training_results = []\n",
    "for p in parameters:\n",
    "    training_results.append(sim_CPMG_G_SA(*p))\n",
    "\n",
    "parameters  =  [(T, M, Omega, n_x, np.random.rand()*2, (2*np.random.rand(n_x)-1), n_max) for n_x,_ in product(range(1,n_max+1),range(25))]\n",
    "testing_results = []\n",
    "for p in parameters:\n",
    "    testing_results.append(sim_CPMG_G_SA(*p))\n",
    "\n",
    "training_inputs  = [np.concatenate([x[0] for x in training_results], 0), np.concatenate([x[1] for x in training_results], 0)]\n",
    "training_targets =  np.concatenate([x[2] for x in training_results], 0)    \n",
    "\n",
    "testing_inputs   = [np.concatenate([x[0] for x in testing_results], 0), np.concatenate([x[1] for x in testing_results], 0)]\n",
    "testing_targets  =  np.concatenate([x[2] for x in testing_results], 0)   \n",
    "\n",
    "# store the dataset externally in a binary pickle file\n",
    "f = open(\"./../datasets/CPMG_G_X_%d.ds\"%n_max, 'wb')\n",
    "pickle.dump({\"T\":T, \"M\":M, \"Omega\":Omega, \"training_inputs\":training_inputs, \"training_targets\":training_targets, \"testing_inputs\":testing_inputs, \"testing_targets\": testing_targets}, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load dataset\n",
    "f = open(\"./../datasets/CPMG_G_X_28.ds\", 'rb')\n",
    "data = pickle.load(f)\n",
    "f.close()  \n",
    "\n",
    "# 2) Load all variables  \n",
    "T                = data[\"T\"]\n",
    "M                = data[\"M\"]\n",
    "Omega            = data[\"Omega\"]\n",
    "training_inputs  = data[\"training_inputs\"]\n",
    "training_targets = data[\"training_targets\"]\n",
    "testing_inputs   = data[\"testing_inputs\"]\n",
    "testing_targets  = data[\"testing_targets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: train a model (skip if the model is already trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)  Define the ML model\n",
    "mlmodel = qubitMLmodel(T/M, Omega, \"Single_Axis\", 2)\n",
    "\n",
    "# 4) Perform training \n",
    "mlmodel.train_model_val(training_inputs, training_targets, testing_inputs, testing_targets, 3000)    \n",
    "\n",
    "# 5) Save results        \n",
    "mlmodel.save_model(\"trained_model_CPMG_G_X_28_3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "if testing_inputs[1].shape[-1] == 1:\n",
    "    mlmodel = qubitMLmodel(T/M, Omega, \"Single_Axis\", testing_inputs[0].shape[-1])\n",
    "else:\n",
    "    mlmodel = qubitMLmodel(T/M, Omega, \"Multi_Axis\", testing_inputs[0].shape[-1]//2)    \n",
    "mlmodel.load_model(\"trained_model_CPMG_G_X_28_3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: display training history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4.8, 3.8])\n",
    "plt.loglog(mlmodel.training_history, label=\"Training\")\n",
    "plt.loglog(mlmodel.val_history, label=\"Validation\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('MSE',fontsize=11)\n",
    "plt.xscale('log')\n",
    "plt.xticks(sum([[i*j for i in range(1,11)] for j in [1,10,100,1000]],[]),fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(True, which=\"both\")\n",
    "\n",
    "# display the final MSE\n",
    "print( \"MSE for training set is %e\"%mlmodel.model.evaluate(training_inputs,training_targets, batch_size = training_targets.shape[0])[0] )\n",
    "print( \"MSE for testing  set is %e\"%mlmodel.model.evaluate(testing_inputs, testing_targets,  batch_size = testing_targets.shape[0])[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: display some testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = np.array([(0.5*T/M) + (j*T/M) for j in range(M)]) # time_domain vector\n",
    "\n",
    "# use the trained model to predict measurements of the training set\n",
    "predicted_measurements_testing  = mlmodel.predict_measurements(testing_inputs)\n",
    "\n",
    "# define a function to display a particular example      \n",
    "def update_display(idx_example):\n",
    "    plt1 = testing_inputs[1][idx_example,:]\n",
    "    plt2 = testing_targets[idx_example,:]\n",
    "    plt3 = predicted_measurements_testing[idx_example,:]\n",
    "\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(time_range, plt1[:,0],'r',label='h_x(t)')\n",
    "    plt.xlabel('t',fontsize=11)\n",
    "    plt.ylabel(\"$\\mathbf{h}(t)$\",fontsize=11)\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=11)\n",
    "        \n",
    "    ax = plt.subplot(2,1,2)\n",
    "    plt.plot(plt2, \".\", label = \"actual\")\n",
    "    plt.plot(plt3, \"x\", label = \"predicted\")\n",
    "    plt.xlabel('Initial State/Measurement Operator',fontsize=11)\n",
    "    plt.xticks(fontsize=11)\n",
    "    labels = [ [r\"$\\rho_{%s}, \\sigma_{%s}$\"%(rho,O) for O in [\"x\",\"y\",\"z\"]] for rho in [\"x+\",\"x-\",\"y+\",\"y-\",\"z+\",\"z-\"]]\n",
    "    labels = sum(labels,[])\n",
    "    ax.set_xticks([x for x in range(18)])\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.ylabel(\"Expectation\",fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=11)   \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a widget for selecting the example\n",
    "widgets.interact(update_display, idx_example=widgets.IntSlider(min=0,max=testing_targets.shape[0]-1,step=1, continuous_update=False) )\n",
    "# or display an example directly \n",
    "#update_display(0)\n",
    "#update_display(381)\n",
    "#update_display(470)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: perform quantum control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 28\n",
    "\n",
    "G_names= [\"I\", \"X\", \"Y\", \"Z\"]     \n",
    "G = [np.array([[1.,0.],[0.,1.]]), np.array([[0.,1.],[1.,0.]]), np.array([[0.,-1j],[1j,0.]]), np.array([[1.,0],[0.,-1.]])]\n",
    "\n",
    "for idx_G, G in enumerate(G):\n",
    "    mlmodel.construct_controller(T, M, n_max)\n",
    "    pulses = mlmodel.train_controller(G,1000)\n",
    "\n",
    "    # plot the results\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(time_range, pulses[1][0,:,0],'r',label='h_x(t)')\n",
    "    plt.xlabel('t',fontsize=11)\n",
    "    plt.ylabel(\"$\\mathbf{h}(t)$\",fontsize=11)\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=11)\n",
    "    print(\"Fidelities for gate %s are %f %f, %f, %f\\n\"%(G_names[idx_G], 100*(1-mlmodel.controller_training_history[\"Fid_0_loss\"][-1]), 100*(1-mlmodel.controller_training_history[\"Fid_1_loss\"][-1]), 100*(1-mlmodel.controller_training_history[\"Fid_2_loss\"][-1]), 100*(1-mlmodel.controller_training_history[\"Fid_3_loss\"][-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
